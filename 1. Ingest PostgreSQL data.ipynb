{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d225e5-5d56-4aba-a62f-8548da097aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "These are the credentials. Always store these in your databricks secrets or key vault!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ee5c9c-5e9d-4dcb-993d-106de1387c76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enhanced function with volume write"
    }
   },
   "outputs": [],
   "source": [
    "def ExtractPostgreSQL(table, mode=\"full\", date_column=None):\n",
    "    # DB connection, always keep this is secrets or key vault!!!\n",
    "    pg_user = \"testdatabase_2ymo_user\"\n",
    "    pg_password = \"0PY1SKhSkJQOJMAdwUd8ypDRt7uJJZS2\"\n",
    "    pg_host = \"dpg-d51vvuer433s73bfnvsg-a.frankfurt-postgres.render.com\"\n",
    "    pg_port = \"5432\"\n",
    "    pg_database = \"testdatabase_2ymo\"\n",
    "\n",
    "    jdbc_url = f\"jdbc:postgresql://{pg_host}:{pg_port}/{pg_database}\"\n",
    "    \n",
    "    # Create a volume\n",
    "    volume_name = table.lower()\n",
    "    volume_path = f\"/Volumes/dev/raw/{volume_name}\"\n",
    "    \n",
    "    # Check if volume exists, create if not\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE VOLUME `dev`.`raw`.`{volume_name}`\")\n",
    "        print(f\"Volume dev.raw.{volume_name} already exists.\")\n",
    "    except Exception:\n",
    "        print(f\"Volume dev.raw.{volume_name} does not exist. Creating...\")\n",
    "        spark.sql(f\"CREATE VOLUME IF NOT EXISTS `dev`.`raw`.`{volume_name}`\")\n",
    "        print(f\"Volume dev.raw.{volume_name} created successfully.\")\n",
    "    \n",
    "    # Build the query based on mode, delta gets the incremental difference compared to the last extracted date\n",
    "    if mode == \"delta\":\n",
    "        if not date_column:\n",
    "            raise ValueError(\"date_column parameter is required for delta mode\")\n",
    "        \n",
    "        # Check if parquet files exist in the volume\n",
    "        try:\n",
    "            files = dbutils.fs.ls(volume_path)\n",
    "            parquet_files = [f for f in files if f.path.endswith('.parquet') or f.isDir()]\n",
    "            \n",
    "            if len(parquet_files) > 0:\n",
    "                existing_df = spark.read.parquet(volume_path)\n",
    "                max_date = existing_df.agg({date_column: \"max\"}).collect()[0][0]\n",
    "                print(f\"Found existing data. Max {date_column}: {max_date}\")\n",
    "                \n",
    "                dbtable = f'(SELECT * FROM public.\"{table}\" WHERE \"{date_column}\" > \\'{max_date}\\') AS new_data'\n",
    "                write_mode = \"append\"\n",
    "            else:\n",
    "                print(\"Volume is empty. Performing full load for first run...\")\n",
    "                dbtable = f'public.\"{table}\"'\n",
    "                write_mode = \"overwrite\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"No existing data found: {e}\")\n",
    "            print(\"Performing full load for first run...\")\n",
    "            dbtable = f'public.\"{table}\"'\n",
    "            write_mode = \"overwrite\"\n",
    "\n",
    "    else:  \n",
    "        dbtable = f'public.\"{table}\"'\n",
    "        write_mode = \"overwrite\"\n",
    "        print(\"Performing full load (overwrite mode)...\")\n",
    "\n",
    "    # Read data from PostgreSQL based on parameters in the earlier code\n",
    "    df = (\n",
    "        spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", jdbc_url)\n",
    "        .option(\"dbtable\", dbtable)\n",
    "        .option(\"user\", pg_user)\n",
    "        .option(\"password\", pg_password)\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "    # Write data to volume\n",
    "    if df.count() > 0:\n",
    "        df.write.mode(write_mode).format(\"parquet\").save(volume_path)\n",
    "        print(f\"Data written to {volume_path} in {write_mode} mode. Rows: {df.count()}\")\n",
    "    else:\n",
    "        print(\"No new data to write.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Full load (overwrite everything)\n",
    "# getdata = ExtractPostgreSQL(\"Customers\", mode=\"full\")\n",
    "\n",
    "# Delta load (append only new records based on date column)\n",
    "getdata = ExtractPostgreSQL(\"Customers\", mode=\"delta\", date_column=\"modified_on\")\n",
    "display(getdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58250c4-74aa-4e09-a3ca-5524b37994fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "118fc514-154b-42a9-88ab-17fdc598de37",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Delete all volumes in dev.raw"
    }
   },
   "outputs": [],
   "source": [
    "# Get all volumes in dev.raw schema\n",
    "try:\n",
    "    volumes_df = spark.sql(\"SHOW VOLUMES IN `dev`.`raw`\")\n",
    "    volumes = [row.volume_name for row in volumes_df.collect()]\n",
    "    \n",
    "    if len(volumes) > 0:\n",
    "        print(f\"Found {len(volumes)} volume(s) to delete: {volumes}\")\n",
    "        \n",
    "        # Drop each volume\n",
    "        for volume in volumes:\n",
    "            print(f\"Dropping volume dev.raw.{volume}...\")\n",
    "            spark.sql(f\"DROP VOLUME IF EXISTS `dev`.`raw`.`{volume}`\")\n",
    "            print(f\"Volume dev.raw.{volume} deleted successfully.\")\n",
    "        \n",
    "        print(f\"\\nAll volumes in dev.raw have been deleted.\")\n",
    "    else:\n",
    "        print(\"No volumes found in dev.raw schema.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure the dev.raw schema exists.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Ingest PostgreSQL data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
