{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ee5c9c-5e9d-4dcb-993d-106de1387c76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "The function that will extract the data from PostgreSQL (either full load or incremental)"
    }
   },
   "outputs": [],
   "source": [
    "def ExtractPostgreSQL(table, mode=\"full\", date_column=None):\n",
    "    # The connection details, which are stored in secret scope\n",
    "    pg_user = dbutils.secrets.get(scope=\"postgres_secrets\", key=\"pg_user\")\n",
    "    pg_password = dbutils.secrets.get(scope=\"postgres_secrets\", key=\"pg_password\")\n",
    "    pg_host = dbutils.secrets.get(scope=\"postgres_secrets\", key=\"pg_host\")\n",
    "    pg_port = dbutils.secrets.get(scope=\"postgres_secrets\", key=\"pg_port\")\n",
    "    pg_database = dbutils.secrets.get(scope=\"postgres_secrets\", key=\"pg_database\")\n",
    "\n",
    "    jdbc_url = f\"jdbc:postgresql://{pg_host}:{pg_port}/{pg_database}\"\n",
    "    \n",
    "    # Create a volume, since that is where files are stored\n",
    "    volume_name = table.lower()\n",
    "    volume_path = f\"/Volumes/dev/bronze/{volume_name}\"\n",
    "    \n",
    "    # Check if the volume does not exist create is, else do nothing\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE VOLUME `dev`.`bronze`.`{volume_name}`\")\n",
    "        print(f\"Volume dev.bronze.{volume_name} already exists.\")\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Volume dev.bronze.{volume_name} does not exist. Creating...\")\n",
    "        spark.sql(f\"CREATE VOLUME IF NOT EXISTS `dev`.`bronze`.`{volume_name}`\")\n",
    "        print(f\"Volume dev.bronze.{volume_name} created successfully.\")\n",
    "    \n",
    "    # Build the query based on mode, delta gets the incremental difference compared to the last extracted date\n",
    "    if mode == \"delta\":\n",
    "        if not date_column:\n",
    "            raise ValueError(\"date_column parameter is required to determine the delta\")\n",
    "        \n",
    "        # Check if parquet files exist in the volume to determine if we want to do a delta or full\n",
    "        try:\n",
    "            files = dbutils.fs.ls(volume_path)\n",
    "            parquet_files = [f for f in files if f.path.endswith('.parquet') or f.isDir()]\n",
    "            \n",
    "            if len(parquet_files) > 0:\n",
    "                existing_df = spark.read.parquet(volume_path)\n",
    "                max_date = existing_df.agg({date_column: \"max\"}).collect()[0][0]\n",
    "                print(f\"Found existing data. Max {date_column}: {max_date}\")\n",
    "                \n",
    "                dbtable = f'(SELECT * FROM public.\"{table}\" WHERE \"{date_column}\" > \\'{max_date}\\') AS new_data'\n",
    "                write_mode = \"append\"\n",
    "\n",
    "            else:\n",
    "                print(\"Volume does not have any data. Performing full load for first run...\")\n",
    "                dbtable = f'public.\"{table}\"'\n",
    "                write_mode = \"append\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"No existing data found: {e}\")\n",
    "            print(\"Performing full load for first run...\")\n",
    "            dbtable = f'public.\"{table}\"'\n",
    "            write_mode = \"overwrite\"\n",
    "\n",
    "    else:  \n",
    "        dbtable = f'public.\"{table}\"'\n",
    "        write_mode = \"overwrite\"\n",
    "        print(\"Performing full load (overwrite mode)...\")\n",
    "\n",
    "    # Read data from PostgreSQL based on parameters in the earlier code\n",
    "    df = (\n",
    "        spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", jdbc_url)\n",
    "        .option(\"dbtable\", dbtable)\n",
    "        .option(\"user\", pg_user)\n",
    "        .option(\"password\", pg_password)\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "    # Write data to volume\n",
    "    if df.count() > 0:\n",
    "        df.write.mode(write_mode).format(\"parquet\").save(volume_path)\n",
    "        print(f\"Data written to {volume_path} in {write_mode} mode. Rows: {df.count()}\")\n",
    "    else:\n",
    "        print(\"No new data to write.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58250c4-74aa-4e09-a3ca-5524b37994fe",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"name\":137,\"email\":201,\"city\":153},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768317319380}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Using the function to extract data"
    }
   },
   "outputs": [],
   "source": [
    "getdata = ExtractPostgreSQL(\"Customers\", mode=\"delta\", date_column=\"modified_on\")\n",
    "display(getdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "118fc514-154b-42a9-88ab-17fdc598de37",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Only used to delete all existing volumes (for testing)"
    }
   },
   "outputs": [],
   "source": [
    "# Get all volumes in dev.raw schema\n",
    "try:\n",
    "    volumes_df = spark.sql(\"SHOW VOLUMES IN `dev`.`raw`\")\n",
    "    volumes = [row.volume_name for row in volumes_df.collect()]\n",
    "    \n",
    "    if len(volumes) > 0:\n",
    "        print(f\"Found {len(volumes)} volume(s) to delete: {volumes}\")\n",
    "        \n",
    "        # Drop each volume\n",
    "        for volume in volumes:\n",
    "            print(f\"Dropping volume dev.raw.{volume}...\")\n",
    "            spark.sql(f\"DROP VOLUME IF EXISTS `dev`.`raw`.`{volume}`\")\n",
    "            print(f\"Volume dev.raw.{volume} deleted successfully.\")\n",
    "        \n",
    "        print(f\"\\nAll volumes in dev.raw have been deleted.\")\n",
    "    else:\n",
    "        print(\"No volumes found in dev.raw schema.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure the dev.raw schema exists.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Ingest PostgreSQL data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
